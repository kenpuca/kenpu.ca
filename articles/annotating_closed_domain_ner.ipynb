{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Approaches and Challenges in Annotating a Closed Domain NER Dataset\"\n",
    "author: Zikun Fu\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities in text into predefined categories such as person names, organizations, locations, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed Domain NER\n",
    "Closed Domain Named Entity Recognition (CD-NER) involves extracting entities from text that correspond to elements of a structured database, such as table names, column names, or partial tuple values. This domain-specific set can contain billions of entities, making extraction a significant challenge. The primary difficulty lies in accurately identifying entities within this closed set while managing the complexities of database size and specificity. CD-NER requires handling specialized vocabulary, leveraging domain-specific context, and dealing with a large fixed pool of entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks\n",
    "In the field of text-to-SQL translation, benchmark datasets like BIRD and Spider have advanced research and established baselines. However, the lack of high-quality CD-NER benchmark datasets limits progress in this area. This article addresses this gap by converting text-to-SQL benchmarks into CD-NER benchmarks. By leveraging structured features from text-to-SQL datasets, we aim to provide a reliable evaluation resource for closed-domain entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD Dataset\n",
    "We'll be working with the BIRD dataset, which contains natural language questions paired with SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Relative project root path\n",
    "project_root = Path(\"../..\")\n",
    "src_path = project_root / \"5-API\" / \"src\"\n",
    "dataset_path = project_root / \"data\" / \"BIRD\"\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(src_path.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 9428\n"
     ]
    }
   ],
   "source": [
    "from cdner.datasets import BirdDataset\n",
    "from cdner.annotators.pglast_annotator import PglastAnnotator\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = BirdDataset(root=dataset_path, train=True).load()\n",
    "examples_list = list(dataset.examples) \n",
    "print(f\"Number of samples in the dataset: {len(examples_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'movie_platform',\n",
      " 'id': 'bird:train.json:0',\n",
      " 'query': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 '\n",
      "          'ORDER BY movie_popularity DESC LIMIT 1',\n",
      " 'question': 'Name movie titles released in year 1945. Sort the listing by the '\n",
      "             'descending order of movie popularity.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Display the first sample\n",
    "pprint(examples_list[0].model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CD-NER Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform BIRD to CDNER, we map `sentence` (question text) to `lexemes` (SQL query entities).\n",
    "\n",
    "This is broken down into the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Lexemes\n",
    "We begin by parsing the SQL queries using `pglast`, a Python library that parses PostgreSQL SQL statements into an Abstract Syntax Tree (AST). This AST representation allows us to navigate the structure of the SQL queries and extract:\n",
    "\n",
    "- `Tables`: Identified by navigating RangeVar nodes in the AST.\n",
    "- `Columns`: Extracted from ColumnRef nodes.\n",
    "- `Values`: Retrieved from A_Const nodes representing constants in the query.\n",
    "\n",
    "This provides a candidate list that needs to be matched with the question text. We call the extracted entities `lexemes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Lexemes\n",
    "Once we have the list of `lexemes`, the next step is to match them with substrings in the corresponding natural language question or `sentence`. \n",
    "Direct string matching is often insufficient due to variations in phrasing, synonyms, or differences in tokenization. \n",
    "To address this, we use a **convolutional search** with **fuzzy string matching**:\n",
    "\n",
    "  - **Tokenization**: The question text is tokenized, preserving the position of each token for accurate mapping.\n",
    "  - **Convolutional Search**: We slide a window over the tokens to consider all possible substrings of varying lengths.\n",
    "  - **Fuzzy Matching**: For each substring, we compute a similarity score with the entity using metrics like the token sort ratio from the thefuzz library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotating the sentence\n",
    "We annotate each `sentence` with:\n",
    "  - **Start and End Positions**: Indicating the exact location of the entity in the question.\n",
    "  - **Label Type**: Denoting whether the entity is a `table`, `column`, or `value`.\n",
    "  - **Lexeme**: The original entity extracted from the SQL query.\n",
    "  - **Similarity Score**: Reflecting the confidence of the match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the BIO Tagging\n",
    "Finally, we convert the annotated entities into a BIO tagging format:\n",
    "\n",
    "- `B-Label`: Marks the beginning of an entity.\n",
    "- `I-Label`: Marks tokens inside an entity.\n",
    "- `O`: Marks tokens outside any entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Lexemes\n",
    "Using `pglast`, we parse the SQL query and extract the following lexemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [{'end': 34,\n",
      "               'label_type': 'column',\n",
      "               'lexeme': 'movie_release_year',\n",
      "               'schema_element': None,\n",
      "               'similarity': 0.77,\n",
      "               'start': 5,\n",
      "               'substring': 'movie titles released in year'},\n",
      "              {'end': 101,\n",
      "               'label_type': 'column',\n",
      "               'lexeme': 'movie_popularity',\n",
      "               'schema_element': None,\n",
      "               'similarity': 1.0,\n",
      "               'start': 85,\n",
      "               'substring': 'movie popularity'},\n",
      "              {'end': 17,\n",
      "               'label_type': 'column',\n",
      "               'lexeme': 'movie_title',\n",
      "               'schema_element': None,\n",
      "               'similarity': 0.96,\n",
      "               'start': 5,\n",
      "               'substring': 'movie titles'},\n",
      "              {'end': 10,\n",
      "               'label_type': 'table',\n",
      "               'lexeme': 'movies',\n",
      "               'schema_element': None,\n",
      "               'similarity': 0.91,\n",
      "               'start': 5,\n",
      "               'substring': 'movie'},\n",
      "              {'end': 39,\n",
      "               'label_type': 'value',\n",
      "               'lexeme': '1945',\n",
      "               'schema_element': None,\n",
      "               'similarity': 1.0,\n",
      "               'start': 35,\n",
      "               'substring': '1945'}],\n",
      " 'id': 'bird:train.json:0',\n",
      " 'question': 'Name movie titles released in year 1945. Sort the listing by the '\n",
      "             'descending order of movie popularity.'}\n"
     ]
    }
   ],
   "source": [
    "annotator = PglastAnnotator()\n",
    "\n",
    "annotated_example = annotator.annotate(examples_list[0])\n",
    "pprint(annotated_example.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Lexemes\n",
    "We use **convolutional search** with **fuzzy matching** to align `lexemes` with segments of the `sentence` (question text). The matching process identifies the most similar substring within a sliding window across the sentence, based on a similarity threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\n",
      "lexemes = [('column', 'movie_release_year')]\n",
      "Starting the matching process:\n",
      "\n",
      "Matching lexeme 'movie_release_year' of type 'column'\n",
      "\n",
      "Searching for best match for phrase 'movie_release_year' in sentence.\n",
      "Window 'Name movie titles released in' (Tokens 0-5): Similarity = 0.64\n",
      "Window 'movie titles released in year' (Tokens 1-6): Similarity = 0.77\n",
      "Window 'titles released in year 1945' (Tokens 2-7): Similarity = 0.61\n",
      "Window 'released in year 1945. Sort' (Tokens 3-8): Similarity = 0.64\n",
      "Window 'in year 1945. Sort the' (Tokens 4-9): Similarity = 0.46\n",
      "Window 'year 1945. Sort the listing' (Tokens 5-10): Similarity = 0.41\n",
      "Window '1945. Sort the listing by' (Tokens 6-11): Similarity = 0.24\n",
      "Window 'Sort the listing by the' (Tokens 7-12): Similarity = 0.29\n",
      "Window 'the listing by the descending' (Tokens 8-13): Similarity = 0.3\n",
      "Window 'listing by the descending order' (Tokens 9-14): Similarity = 0.29\n",
      "Window 'by the descending order of' (Tokens 10-15): Similarity = 0.32\n",
      "Window 'the descending order of movie' (Tokens 11-16): Similarity = 0.43\n",
      "Window 'descending order of movie popularity' (Tokens 12-17): Similarity = 0.41\n",
      "Window size 5: Best match 'movie titles released in year' with similarity 0.77\n",
      "Window 'Name movie titles released' (Tokens 0-4): Similarity = 0.68\n",
      "Window 'movie titles released in' (Tokens 1-5): Similarity = 0.71\n",
      "Window 'titles released in year' (Tokens 2-6): Similarity = 0.68\n",
      "Window 'released in year 1945' (Tokens 3-7): Similarity = 0.72\n",
      "Window 'in year 1945. Sort' (Tokens 4-8): Similarity = 0.46\n",
      "Window 'year 1945. Sort the' (Tokens 5-9): Similarity = 0.44\n",
      "Window '1945. Sort the listing' (Tokens 6-10): Similarity = 0.26\n",
      "Window 'Sort the listing by' (Tokens 7-11): Similarity = 0.27\n",
      "Window 'the listing by the' (Tokens 8-12): Similarity = 0.33\n",
      "Window 'listing by the descending' (Tokens 9-13): Similarity = 0.28\n",
      "Window 'by the descending order' (Tokens 10-14): Similarity = 0.34\n",
      "Window 'the descending order of' (Tokens 11-15): Similarity = 0.29\n",
      "Window 'descending order of movie' (Tokens 12-16): Similarity = 0.42\n",
      "Window 'order of movie popularity' (Tokens 13-17): Similarity = 0.51\n",
      "Window size 4: Best match 'released in year 1945' with similarity 0.72\n",
      "Window 'Name movie titles' (Tokens 0-3): Similarity = 0.57\n",
      "Window 'movie titles released' (Tokens 1-4): Similarity = 0.77\n",
      "Window 'titles released in' (Tokens 2-5): Similarity = 0.61\n",
      "Window 'released in year' (Tokens 3-6): Similarity = 0.82\n",
      "Window 'in year 1945' (Tokens 4-7): Similarity = 0.4\n",
      "Window 'year 1945. Sort' (Tokens 5-8): Similarity = 0.44\n",
      "Window '1945. Sort the' (Tokens 6-9): Similarity = 0.26\n",
      "Window 'Sort the listing' (Tokens 7-10): Similarity = 0.29\n",
      "Window 'the listing by' (Tokens 8-11): Similarity = 0.31\n",
      "Window 'listing by the' (Tokens 9-12): Similarity = 0.31\n",
      "Window 'by the descending' (Tokens 10-13): Similarity = 0.34\n",
      "Window 'the descending order' (Tokens 11-14): Similarity = 0.32\n",
      "Window 'descending order of' (Tokens 12-15): Similarity = 0.32\n",
      "Window 'order of movie' (Tokens 13-16): Similarity = 0.56\n",
      "Window 'of movie popularity' (Tokens 14-17): Similarity = 0.49\n",
      "Window size 3: Best match 'released in year' with similarity 0.82\n",
      "Window 'Name movie' (Tokens 0-2): Similarity = 0.57\n",
      "Window 'movie titles' (Tokens 1-3): Similarity = 0.6\n",
      "Window 'titles released' (Tokens 2-4): Similarity = 0.55\n",
      "Window 'released in' (Tokens 3-5): Similarity = 0.62\n",
      "Window 'in year' (Tokens 4-6): Similarity = 0.48\n",
      "Window 'year 1945' (Tokens 5-7): Similarity = 0.37\n",
      "Window '1945. Sort' (Tokens 6-8): Similarity = 0.22\n",
      "Window 'Sort the' (Tokens 7-9): Similarity = 0.31\n",
      "Window 'the listing' (Tokens 8-10): Similarity = 0.28\n",
      "Window 'listing by' (Tokens 9-11): Similarity = 0.21\n",
      "Window 'by the' (Tokens 10-12): Similarity = 0.17\n",
      "Window 'the descending' (Tokens 11-13): Similarity = 0.31\n",
      "Window 'descending order' (Tokens 12-14): Similarity = 0.35\n",
      "Window 'order of' (Tokens 13-15): Similarity = 0.38\n",
      "Window 'of movie' (Tokens 14-16): Similarity = 0.46\n",
      "Window 'movie popularity' (Tokens 15-17): Similarity = 0.53\n",
      "Window size 2: Best match 'released in' with similarity 0.62\n",
      "Window 'Name' (Tokens 0-1): Similarity = 0.18\n",
      "Window 'movie' (Tokens 1-2): Similarity = 0.43\n",
      "Window 'titles' (Tokens 2-3): Similarity = 0.33\n",
      "Window 'released' (Tokens 3-4): Similarity = 0.54\n",
      "Window 'in' (Tokens 4-5): Similarity = 0.1\n",
      "Window 'year' (Tokens 5-6): Similarity = 0.36\n",
      "Window '1945' (Tokens 6-7): Similarity = 0.0\n",
      "Window 'Sort' (Tokens 7-8): Similarity = 0.18\n",
      "Window 'the' (Tokens 8-9): Similarity = 0.1\n",
      "Window 'listing' (Tokens 9-10): Similarity = 0.16\n",
      "Window 'by' (Tokens 10-11): Similarity = 0.1\n",
      "Window 'the' (Tokens 11-12): Similarity = 0.1\n",
      "Window 'descending' (Tokens 12-13): Similarity = 0.21\n",
      "Window 'order' (Tokens 13-14): Similarity = 0.35\n",
      "Window 'of' (Tokens 14-15): Similarity = 0.1\n",
      "Window 'movie' (Tokens 15-16): Similarity = 0.43\n",
      "Window 'popularity' (Tokens 16-17): Similarity = 0.29\n",
      "Window size 1: Best match 'released' with similarity 0.54\n",
      "\n",
      "Best overall match: 'released in year' with similarity 0.82\n",
      "Matched 'released in year' in sentence with similarity 0.82\n",
      "\n",
      "Matched entities:\n",
      "Entity Type: column\n",
      "Matched Text: 'released in year'\n",
      "Lexeme: movie_release_year\n",
      "Similarity: 0.82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\"\n",
    "lexemes = [\n",
    "    ('column', 'movie_release_year'),\n",
    "    # ('column', 'movie_popularity'),\n",
    "    # ('column', 'movie_title'),\n",
    "    # ('table', 'movies'),\n",
    "    # ('value', '1945')\n",
    "]\n",
    "print(\"Sentence =\",sentence)\n",
    "print(\"lexemes =\", lexemes)\n",
    "# Set a similarity threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Perform matching\n",
    "print(\"Starting the matching process:\")\n",
    "entities = conv_match_substring(sentence, lexemes, threshold=threshold)\n",
    "\n",
    "# Display the matched entities\n",
    "print(\"\\nMatched entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"Entity Type: {entity.label_type}\")\n",
    "    print(f\"Matched Text: '{sentence[entity.start:entity.end]}'\")\n",
    "    print(f\"Lexeme: {entity.lexeme}\")\n",
    "    print(f\"Similarity: {entity.similarity}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "- **Alignment Issues**:\n",
    "    - Natural language questions often use varied phrasing that doesn't directly match the `lexemes` (e.g., table names, column names) in the database schema. \n",
    "- **Overlapping Entities**:\n",
    "    - When multiple entities are mentioned closely together in a question, their textual representations can overlap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approach the challenge in the following ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alignment\n",
    "    1. **Continous Annotation**: Only continuous (adjacent) substrings in the sentence can be annotated as entities. This means that the words corresponding to an entity must be next to each other without any interruptions.\n",
    "    2. **Non-continuous Annotation**: Allows for the annotation of entities even if the corresponding words are not adjacent in the sentence. This approach is more flexible and can capture entities that are mentioned in a scattered manner throughout the sentence.\n",
    "- Overlap\n",
    "    1. **Overlap Annotation**: Annotations are allowed to overlap in the sentence; that is, a word or phrase can be part of multiple entity annotations. This is useful when different entities share common words in the question.\n",
    "    2. **Non-overlap Annotation**: Annotations cannot overlap; each word or phrase can be assigned to at most one entity. This constraint ensures that once a word is part of an entity annotation, it cannot be part of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Suppose the following `sentence` and `lexemes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\"\n",
    "lexemes = [\n",
    "    ('column', 'movie_release_year'),\n",
    "    ('column', 'movie_popularity'),\n",
    "    ('column', 'movie_title'),\n",
    "    ('table', 'movies'),\n",
    "    ('value', '1945')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Overlapping\n",
    "**Definition:** Only continuous substrings in the sentence can be annotated as entities, and annotations are allowed to overlap (i.e., a word or phrase can be part of multiple annotations).\n",
    "\n",
    "**Sentence:** \"Name `[{movie} titles]` `[released in year]` `[1945]`. Sort the listing by the descending order of `[movie popularity]`.\"\n",
    "\n",
    "- `{movie}` and `[movie released in year]` matches to the 'movie_release_year' column.\n",
    "- `[movie titles]` matches the 'movie_title' column.\n",
    "- `[1945]` matches the '1945' value.\n",
    "- `[movie popularity]` matches the 'movie_popularity' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Non-Overlapping\n",
    "**Definition:** Only continuous substrings in the sentence can be annotated as entities, and annotations cannot overlap (i.e., each word or phrase can be part of at most one annotation).\n",
    "\n",
    "**Sentence:** \"Name `[movie titles]` `[released in year]` `[1945]`. Sort the listing by the descending order of `[movie popularity]`.\"\n",
    "\n",
    "- `[movie titles]` matches the 'movie_title' column.\n",
    "- `[released in year]` matches the 'movie_release_year' column.\n",
    "- `[1945]` matches the '1945' value.\n",
    "- `[movie popularity]` matches the 'movie_popularity' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Continuous Overlapping\n",
    "**Definition:** Substrings can be non-continuous (i.e., words corresponding to an entity do not need to be adjacent), and annotations are allowed to overlap.\n",
    "\n",
    "**Sentence:** \"Name `[{movie} titles]` `[released]` in `[year]` `[1945]`. Sort the listing by the descending order of `[{movie} popularity]`.\"\n",
    "\n",
    "- `{movie}` matches the 'movies' table.\n",
    "- `[movie titles]` separately match the 'movie_title' columns.\n",
    "- `{movie}`, `[released]` and `[year]` correspond to the 'movie_release_year' columns.\n",
    "- `[1945]` matches the '1945' value.\n",
    "- `[movie popularity]` correspond to the 'movie_popularity' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Continuous Non-Overlapping\n",
    "**Definition:** Substrings can be non-continuous, and annotations cannot overlap.\n",
    "\n",
    "**Sentence:** \"Name `[movie]` `[titles]` `[released]` in `[year]` `[1945]`. Sort the listing by the descending order of `[movie]` `[popularity]`.\"\n",
    "\n",
    "- `[movie]` matches the 'movies' table or 'movie_title' column.\n",
    "- `[titles]` matches the 'movie_title' column.\n",
    "- `[released]` matches the 'movie_release_year' column.\n",
    "- `[year]` matches the 'movie_release_year' column.\n",
    "- `[1945]` matches the '1945' value.\n",
    "- `[movie]` `[popularity]` matches the 'movie_popularity' column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
