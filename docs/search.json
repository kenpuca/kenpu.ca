[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Recent Publications",
    "section": "",
    "text": "Nargesian, Fatemeh, Ken Qian Pu, Bahar Ghadiri Bashardoost, Erkang Zhu, and Renee J. Miller. “Data Lake Organization.” IEEE Transactions on Knowledge and Data Engineering (2022). PDF"
  },
  {
    "objectID": "publications.html#journals",
    "href": "publications.html#journals",
    "title": "Recent Publications",
    "section": "",
    "text": "Nargesian, Fatemeh, Ken Qian Pu, Bahar Ghadiri Bashardoost, Erkang Zhu, and Renee J. Miller. “Data Lake Organization.” IEEE Transactions on Knowledge and Data Engineering (2022). PDF"
  },
  {
    "objectID": "publications.html#conferences-workshops",
    "href": "publications.html#conferences-workshops",
    "title": "Recent Publications",
    "section": "2 Conferences & Workshops",
    "text": "2 Conferences & Workshops\n\nLimin Ma and Ken Q. Pu. “Accelerating Relational Keyword Queries With Embedded Predictive Neural Networks.” 2024 IEEE 25th International Conference on Information Reuse and Integration for Data Science (IRI). IEEE Computer Society, 2024.\nMekael Wasti, Ken Q. Pu and Ali Neshati. “Voice Interactive User Interfaces Powered by LLMs.” Intelligent Systems Conference (IntelliSys) 2024.\nPu, Ken, and Limin Ma. “Incremental Computation of Information Gain in Temporal Relational Streams.” 2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI). IEEE Computer Society, 2022.\nValdron, Michael, and Pu, Ken. “Data Driven Relational Constraint Programming.” 2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI). IEEE Computer Society, 2020. PDF\n\n\nA complete list can be found on Google Scholar."
  },
  {
    "objectID": "projects/2024-cdner-dataset.html",
    "href": "projects/2024-cdner-dataset.html",
    "title": "Closed-domain NER Dataset",
    "section": "",
    "text": "1 Introduction\nNamed Entity Recognition (NER) is a core problem in natural language processing that involves identifying and categorizing entities within a text into predefined types, such as persons, organizations, locations, etc. The challenge with NER lies in its open-world nature, where each entity type is assumed to have an unconstrained domain, meaning there is no reliance on pre-existing domain knowledge for these entities. This makes the task complex, as the model must generalize to entities it has never encountered before, without any specific constraints regarding the entities’ characteristics or context. The fixed number of entity types provides structure to the problem, but the open-world assumption significantly increases the difficulty of accurately identifying and classifying diverse and novel entities as they appear in real-world texts.\nClosed-Domain NER Challenge: In this research, we assume a closed domain scenario, where each entity type is associated with a database that provides all possible values of the entity. This changes the nature of the NER problem, as the model now has access to a comprehensive list of potential entities for each type, allowing it to leverage this additional information for more accurate tagging. The key research challenge lies in designing an NER tagger that effectively incorporates this closed-domain constraint. Unlike open-domain systems that must rely on statistical patterns and contextual cues alone, the closed-domain approach requires the integration of explicit entity databases, which brings its own set of complexities. The research question centers on how to best combine traditional sequence labeling methods with database lookups, ensuring that the model benefits from the completeness of the closed domain without compromising flexibility or performance.\n\n\n2 CD-NER Dataset\nDataset Creation for Closed-Domain NER (CD-NER): To evaluate potential solutions for the closed-domain NER (CD-NER) problem, it is essential to create a dedicated dataset that reflects the characteristics of a closed domain. This dataset should include texts that feature entities drawn from a well-defined set of types, each with an exhaustive list of possible values provided by associated databases. The dataset must be curated to represent the range of variability within the domain, capturing different contexts in which the entities may appear, while ensuring that all entities are drawn from the closed set. Additionally, the dataset should be split into training, validation, and test sets, with careful attention given to ensuring that no unseen entities appear during testing, which aligns with the closed-domain assumption. Such a dataset will serve as a benchmark to evaluate the effectiveness of CD-NER models in leveraging pre-existing entity databases for improved recognition and categorization accuracy.\n\n\n\n\n\nAI Automation Group, 2024 ©"
  },
  {
    "objectID": "articles/constraint-modeling.html",
    "href": "articles/constraint-modeling.html",
    "title": "Structured Constraint Programming",
    "section": "",
    "text": "Let’s look at ways we can build constraint programs (CP) in a structured way. As a case study, we will model the curriculum requirements of the Computer Science program."
  },
  {
    "objectID": "articles/constraint-modeling.html#introduction",
    "href": "articles/constraint-modeling.html#introduction",
    "title": "Structured Constraint Programming",
    "section": "",
    "text": "Let’s look at ways we can build constraint programs (CP) in a structured way. As a case study, we will model the curriculum requirements of the Computer Science program."
  },
  {
    "objectID": "articles/constraint-modeling.html#data",
    "href": "articles/constraint-modeling.html#data",
    "title": "Structured Constraint Programming",
    "section": "2 Data",
    "text": "2 Data\nLet’s start with data. The environment is modeled as a database. Let’s build a database to model the curriculum.\n\ndata = []\nfor i in range(4):\n    for s in ['Fall', 'Winter']:\n        data.append(f\"Y{i+1} {s}\")\n\nsemesters = Series(data, name='semesters')\nsemesters\n\n0      Y1 Fall\n1    Y1 Winter\n2      Y2 Fall\n3    Y2 Winter\n4      Y3 Fall\n5    Y3 Winter\n6      Y4 Fall\n7    Y4 Winter\nName: semesters, dtype: object\n\n\n\n\nThe semesters of the curriculum.\n\ndata = [\n    'CSCI 1030U', 'CSCI 1060U', 'CSCI 1061U', 'CSCI 1062U', 'CSCI 1063U', \n    'CSCI 2000U', 'CSCI 2010U', 'CSCI 2020U', 'CSCI 2040U', 'CSCI 2050U',\n    'CSCI 2072U', 'CSCI 2110U', 'CSCI 3010U', 'CSCI 3030U', 'CSCI 3055U', \n    'CSCI 3060U', 'CSCI 3070U', 'CSCI 3090U', 'CSCI 3230U', 'CSCI 3240U', \n    'CSCI 3310U', 'CSCI 3540U', 'CSCI 4020U', 'CSCI 4030U', 'CSCI 4040U', \n    'CSCI 4050U', 'CSCI 4052U', 'CSCI 4055U', 'CSCI 4060U', 'CSCI 4080U', \n    'CSCI 4100U', 'CSCI 4110U', 'CSCI 4140U', 'CSCI 4150U', 'CSCI 4160U', \n    'CSCI 4210U', 'CSCI 4220U', 'CSCI 4230U', 'CSCI 4410U', 'CSCI 4420U', \n    'CSCI 4430U', 'CSCI 4610U', 'CSCI 4620U'\n]\n\ncourses = Series(data, name='courses')\ncourses.head()\n\n0    CSCI 1030U\n1    CSCI 1060U\n2    CSCI 1061U\n3    CSCI 1062U\n4    CSCI 1063U\nName: courses, dtype: object\n\n\nData is needed to model prerequisites and areas of senior courses. For now, let’s get back to these relations laters."
  },
  {
    "objectID": "articles/constraint-modeling.html#unknowns",
    "href": "articles/constraint-modeling.html#unknowns",
    "title": "Structured Constraint Programming",
    "section": "3 Unknowns",
    "text": "3 Unknowns\nWe will also call them independent variables. These are the variables that should be solved to derive the desired solution.\n\ndef make_unknowns(model:CpModel)-&gt;DataFrame:\n    data = np.empty((len(courses), len(semesters)), dtype=object)\n    for i,c in enumerate(courses):\n        for (j, s) in enumerate(semesters):\n            data[i,j] = model.new_bool_var(f\"{c}∈{s}?\")\n\n    unknown = pd.DataFrame(data, index=courses, columns=semesters)\n    return unknown\n\n\nmodel = CpModel()\nunknown = make_unknowns(model)\nunknown.head()\n\n\n\n\n\n\n\nsemesters\nY1 Fall\nY1 Winter\nY2 Fall\nY2 Winter\nY3 Fall\nY3 Winter\nY4 Fall\nY4 Winter\n\n\ncourses\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 1030U\nCSCI 1030U∈Y1 Fall?\nCSCI 1030U∈Y1 Winter?\nCSCI 1030U∈Y2 Fall?\nCSCI 1030U∈Y2 Winter?\nCSCI 1030U∈Y3 Fall?\nCSCI 1030U∈Y3 Winter?\nCSCI 1030U∈Y4 Fall?\nCSCI 1030U∈Y4 Winter?\n\n\nCSCI 1060U\nCSCI 1060U∈Y1 Fall?\nCSCI 1060U∈Y1 Winter?\nCSCI 1060U∈Y2 Fall?\nCSCI 1060U∈Y2 Winter?\nCSCI 1060U∈Y3 Fall?\nCSCI 1060U∈Y3 Winter?\nCSCI 1060U∈Y4 Fall?\nCSCI 1060U∈Y4 Winter?\n\n\nCSCI 1061U\nCSCI 1061U∈Y1 Fall?\nCSCI 1061U∈Y1 Winter?\nCSCI 1061U∈Y2 Fall?\nCSCI 1061U∈Y2 Winter?\nCSCI 1061U∈Y3 Fall?\nCSCI 1061U∈Y3 Winter?\nCSCI 1061U∈Y4 Fall?\nCSCI 1061U∈Y4 Winter?\n\n\nCSCI 1062U\nCSCI 1062U∈Y1 Fall?\nCSCI 1062U∈Y1 Winter?\nCSCI 1062U∈Y2 Fall?\nCSCI 1062U∈Y2 Winter?\nCSCI 1062U∈Y3 Fall?\nCSCI 1062U∈Y3 Winter?\nCSCI 1062U∈Y4 Fall?\nCSCI 1062U∈Y4 Winter?\n\n\nCSCI 1063U\nCSCI 1063U∈Y1 Fall?\nCSCI 1063U∈Y1 Winter?\nCSCI 1063U∈Y2 Fall?\nCSCI 1063U∈Y2 Winter?\nCSCI 1063U∈Y3 Fall?\nCSCI 1063U∈Y3 Winter?\nCSCI 1063U∈Y4 Fall?\nCSCI 1063U∈Y4 Winter?"
  },
  {
    "objectID": "articles/constraint-modeling.html#constraints",
    "href": "articles/constraint-modeling.html#constraints",
    "title": "Structured Constraint Programming",
    "section": "4 Constraints",
    "text": "4 Constraints\nWe can immediate declare some basic constraints.\n\nEach course can only be taken at most once.\nEach semester can have at most 5 courses.\nMust take lots of courses.\n\n\ndef make_taken_atmost_once(model:CpModel, unknown:DataFrame)-&gt;Series:\n    def fn(row:pd.Series):\n        c = sum(row) &lt;= 1\n        model.Add(c)\n        return c\n\n    return unknown.apply(fn, axis=1)\n\n\n\nEach course can only be taken at most once.\n\nmodel = CpModel()\nunknown = make_unknowns(model)\nC1 = make_taken_atmost_once(model, unknown)\nC1.head()\n\ncourses\nCSCI 1030U    (((((((CSCI 1030U∈Y1 Fall? + CSCI 1030U∈Y1 Win...\nCSCI 1060U    (((((((CSCI 1060U∈Y1 Fall? + CSCI 1060U∈Y1 Win...\nCSCI 1061U    (((((((CSCI 1061U∈Y1 Fall? + CSCI 1061U∈Y1 Win...\nCSCI 1062U    (((((((CSCI 1062U∈Y1 Fall? + CSCI 1062U∈Y1 Win...\nCSCI 1063U    (((((((CSCI 1063U∈Y1 Fall? + CSCI 1063U∈Y1 Win...\ndtype: object\n\n\n\ndef make_semester_atmost_five(model:CpModel, unknown:DataFrame)-&gt;Series:\n    def fn(col:pd.Series):\n        c = sum(col) &lt;= 5\n        model.Add(c)\n        return c\n\n    return unknown.apply(fn, axis=0)\n\n\n\nEach semester can only have at most five courses.\n\nmodel = CpModel()\nunknown = make_unknowns(model)\nC1 = make_semester_atmost_five(model, unknown)\nC1.head()\n\nsemesters\nY1 Fall      ((((((((((((((((((((((((((((((((((((((((((CSCI...\nY1 Winter    ((((((((((((((((((((((((((((((((((((((((((CSCI...\nY2 Fall      ((((((((((((((((((((((((((((((((((((((((((CSCI...\nY2 Winter    ((((((((((((((((((((((((((((((((((((((((((CSCI...\nY3 Fall      ((((((((((((((((((((((((((((((((((((((((((CSCI...\ndtype: object\n\n\n\ndef make_min_selection(model:CpModel, unknown:DataFrame, min:int):\n    vars = unknown.values.reshape(-1)\n    c = sum(vars) &gt; min\n    model.Add(c)\n    return c"
  },
  {
    "objectID": "articles/constraint-modeling.html#dependent-variables",
    "href": "articles/constraint-modeling.html#dependent-variables",
    "title": "Structured Constraint Programming",
    "section": "5 Dependent Variables",
    "text": "5 Dependent Variables\nWe will declare a number of dependent variables. These values are derived from data and unkowns (and maybe other dependent variables). Since the values of unknowns are non-deterministic, derived qualities are also variables.\nThey can be general integer variables.\nLet’s create a set of dependent integer variables, taken_in, which indicates the semester that the courses are taken in. The taken_in[c] is from 1 to \\(n\\) if the course [c] is taken. Otherwise taken_in[c] = 0.\n\ndef make_taken_in(model:CpModel, unknown:DataFrame)-&gt;Series:\n    def fn(row:pd.Series)-&gt;cp_model.IntVar:\n        var = model.NewIntVar(0, len(row)+1, 'taken_in')\n        model.add_map_domain(var, row, offset=1)\n        return var\n\n    taken_in = unknown.apply(fn, axis=1)\n    return taken_in\n\n\nmodel = CpModel()\nunknowns = make_unknowns(model)\ntaken_in = make_taken_in(model, unknown)\n\ntaken_in.head()\n\ncourses\nCSCI 1030U    taken_in\nCSCI 1060U    taken_in\nCSCI 1061U    taken_in\nCSCI 1062U    taken_in\nCSCI 1063U    taken_in\ndtype: object\n\n\nLet’s also define the set of dependent variables, taken, which are booleans indicating of the course is taken.\n\ndef make_taken(model:CpModel, unknown:DataFrame)-&gt;Series:\n    def fn(row:pd.Series)-&gt;cp_model.IntVar:\n        var = model.NewBoolVar('taken')\n        model.AddMaxEquality(var, row)\n        return var\n\n    taken = unknown.apply(fn, axis=1)\n    return taken\n\n\nmodel = CpModel()\nunknown = make_unknowns(model)\ntaken = make_taken(model, unknown)\n\ntaken.head()\n\ncourses\nCSCI 1030U    taken\nCSCI 1060U    taken\nCSCI 1061U    taken\nCSCI 1062U    taken\nCSCI 1063U    taken\ndtype: object"
  },
  {
    "objectID": "articles/constraint-modeling.html#solution",
    "href": "articles/constraint-modeling.html#solution",
    "title": "Structured Constraint Programming",
    "section": "6 Solution",
    "text": "6 Solution\nWe can solve the unknowns (hopefully) and the derived variables using a Solver. The solution will be rendered by views.\n\nmodel = CpModel()\nunknown = make_unknowns(model)\n\n#\n# constraints\n#\ntaken_atmost_once = make_taken_atmost_once(model, unknown)\nsemester_atmost_five = make_semester_atmost_five(model, unknown)\n\n#\n# dependent variables\n#\ntaken_in = make_taken_in(model, unknown)\ntaken = make_taken(model, unknown)\nmake_min_selection(model, unknown, min=35)\n\n#\n# solution\n#\nsolver = cp_model.CpSolver()\nstatus = solver.solve(model)\n\nstatus_name = {\n    cp_model.OPTIMAL: 'optimal',\n    cp_model.FEASIBLE: 'feasible',\n    cp_model.INFEASIBLE: 'infeasible',\n    cp_model.MODEL_INVALID: 'invalid',\n    cp_model.UNKNOWN: 'unknown',\n}[status]\n\nstatus_name\n\n'optimal'"
  },
  {
    "objectID": "articles/constraint-modeling.html#viewing-the-solution",
    "href": "articles/constraint-modeling.html#viewing-the-solution",
    "title": "Structured Constraint Programming",
    "section": "7 Viewing the solution",
    "text": "7 Viewing the solution\n\ndef view(solver:CpSolver, df:DataFrame)-&gt;DataFrame:\n    def fn(x):\n        return solver.value(x)\n    return df.map(fn)\n\n\nview(solver, unknown).head()\n\n\n\n\n\n\n\nsemesters\nY1 Fall\nY1 Winter\nY2 Fall\nY2 Winter\nY3 Fall\nY3 Winter\nY4 Fall\nY4 Winter\n\n\ncourses\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 1030U\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nCSCI 1060U\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCSCI 1061U\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCSCI 1062U\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCSCI 1063U\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nview(solver, taken_in)\n\ncourses\nCSCI 1030U    1\nCSCI 1060U    0\nCSCI 1061U    0\nCSCI 1062U    0\nCSCI 1063U    1\nCSCI 2000U    1\nCSCI 2010U    1\nCSCI 2020U    1\nCSCI 2040U    2\nCSCI 2050U    2\nCSCI 2072U    2\nCSCI 2110U    2\nCSCI 3010U    2\nCSCI 3030U    3\nCSCI 3055U    3\nCSCI 3060U    3\nCSCI 3070U    3\nCSCI 3090U    3\nCSCI 3230U    4\nCSCI 3240U    4\nCSCI 3310U    4\nCSCI 3540U    4\nCSCI 4020U    4\nCSCI 4030U    5\nCSCI 4040U    5\nCSCI 4050U    5\nCSCI 4052U    5\nCSCI 4055U    5\nCSCI 4060U    6\nCSCI 4080U    6\nCSCI 4100U    6\nCSCI 4110U    6\nCSCI 4140U    6\nCSCI 4150U    7\nCSCI 4160U    7\nCSCI 4210U    7\nCSCI 4220U    7\nCSCI 4230U    7\nCSCI 4410U    8\nCSCI 4420U    8\nCSCI 4430U    8\nCSCI 4610U    8\nCSCI 4620U    8\ndtype: int64\n\n\n\nview(solver, taken_in) \\\n.to_frame() \\\n.reset_index() \\\n.rename(columns={0: 'semester'}) \\\n.groupby(by='semester') \\\n.agg({\n    'courses': lambda x: (f\"{len(x)} :\" + \", \".join(x))\n})\n\n\n\n\n\n\n\n\ncourses\n\n\nsemester\n\n\n\n\n\n0\n3 :CSCI 1060U, CSCI 1061U, CSCI 1062U\n\n\n1\n5 :CSCI 1030U, CSCI 1063U, CSCI 2000U, CSCI 20...\n\n\n2\n5 :CSCI 2040U, CSCI 2050U, CSCI 2072U, CSCI 21...\n\n\n3\n5 :CSCI 3030U, CSCI 3055U, CSCI 3060U, CSCI 30...\n\n\n4\n5 :CSCI 3230U, CSCI 3240U, CSCI 3310U, CSCI 35...\n\n\n5\n5 :CSCI 4030U, CSCI 4040U, CSCI 4050U, CSCI 40...\n\n\n6\n5 :CSCI 4060U, CSCI 4080U, CSCI 4100U, CSCI 41...\n\n\n7\n5 :CSCI 4150U, CSCI 4160U, CSCI 4210U, CSCI 42...\n\n\n8\n5 :CSCI 4410U, CSCI 4420U, CSCI 4430U, CSCI 46..."
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "",
    "text": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities in text into predefined categories such as person names, organizations, locations, etc.\n\n\n\nClosed Domain Named Entity Recognition (CD-NER) involves extracting entities from text that correspond to elements of a structured database, such as table names, column names, or partial tuple values. This domain-specific set can contain billions of entities, making extraction a significant challenge. The primary difficulty lies in accurately identifying entities within this closed set while managing the complexities of database size and specificity. CD-NER requires handling specialized vocabulary, leveraging domain-specific context, and dealing with a large fixed pool of entities.\n\n\n\nIn the field of text-to-SQL translation, benchmark datasets like BIRD and Spider have advanced research and established baselines. However, the lack of high-quality CD-NER benchmark datasets limits progress in this area. This article addresses this gap by converting text-to-SQL benchmarks into CD-NER benchmarks. By leveraging structured features from text-to-SQL datasets, we aim to provide a reliable evaluation resource for closed-domain entity extraction."
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#introduction",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#introduction",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "",
    "text": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities in text into predefined categories such as person names, organizations, locations, etc.\n\n\n\nClosed Domain Named Entity Recognition (CD-NER) involves extracting entities from text that correspond to elements of a structured database, such as table names, column names, or partial tuple values. This domain-specific set can contain billions of entities, making extraction a significant challenge. The primary difficulty lies in accurately identifying entities within this closed set while managing the complexities of database size and specificity. CD-NER requires handling specialized vocabulary, leveraging domain-specific context, and dealing with a large fixed pool of entities.\n\n\n\nIn the field of text-to-SQL translation, benchmark datasets like BIRD and Spider have advanced research and established baselines. However, the lack of high-quality CD-NER benchmark datasets limits progress in this area. This article addresses this gap by converting text-to-SQL benchmarks into CD-NER benchmarks. By leveraging structured features from text-to-SQL datasets, we aim to provide a reliable evaluation resource for closed-domain entity extraction."
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#bird-dataset",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#bird-dataset",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "2 BIRD Dataset",
    "text": "2 BIRD Dataset\nWe’ll be working with the BIRD dataset, which contains natural language questions paired with SQL queries.\n\nimport sys\nfrom pathlib import Path\n\n# Relative project root path\nproject_root = Path(\"../..\")\nsrc_path = project_root / \"5-API\" / \"src\"\ndataset_path = project_root / \"data\" / \"BIRD\"\n\n# Add src directory to Python path\nsys.path.append(str(src_path.resolve()))\n\n\nfrom cdner.datasets import BirdDataset\nfrom cdner.annotators.pglast_annotator import PglastAnnotator\n\n# Initialize the dataset\ndataset = BirdDataset(root=dataset_path, train=True).load()\nexamples_list = list(dataset.examples) \nprint(f\"Number of samples in the dataset: {len(examples_list)}\")\n\nNumber of samples in the dataset: 9428\n\n\n\nfrom pprint import pprint\n# Display the first sample\npprint(examples_list[0].model_dump())\n\n{'db_id': 'movie_platform',\n 'id': 'bird:train.json:0',\n 'query': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 '\n          'ORDER BY movie_popularity DESC LIMIT 1',\n 'question': 'Name movie titles released in year 1945. Sort the listing by the '\n             'descending order of movie popularity.'}"
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#building-cd-ner-benchmarks",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#building-cd-ner-benchmarks",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "3 Building CD-NER Benchmarks",
    "text": "3 Building CD-NER Benchmarks\nTo transform BIRD to CDNER, we map sentence (question text) to lexemes (SQL query entities).\nThis is broken down into the following steps:\n\n3.1 Extracting Lexemes\nWe begin by parsing the SQL queries using pglast, a Python library that parses PostgreSQL SQL statements into an Abstract Syntax Tree (AST). This AST representation allows us to navigate the structure of the SQL queries and extract:\n\nTables: Identified by navigating RangeVar nodes in the AST.\nColumns: Extracted from ColumnRef nodes.\nValues: Retrieved from A_Const nodes representing constants in the query.\n\nThis provides a candidate list that needs to be matched with the question text. We call the extracted entities lexemes.\n\n\n3.2 Matching Lexemes\nOnce we have the list of lexemes, the next step is to match them with substrings in the corresponding natural language question or sentence. Direct string matching is often insufficient due to variations in phrasing, synonyms, or differences in tokenization. To address this, we use a convolutional search with fuzzy string matching:\n\nTokenization: The question text is tokenized, preserving the position of each token for accurate mapping.\nConvolutional Search: We slide a window over the tokens to consider all possible substrings of varying lengths.\nFuzzy Matching: For each substring, we compute a similarity score with the entity using metrics like the token sort ratio from the thefuzz library.\n\n\n\n3.3 Annotating the sentence\nWe annotate each sentence with: - Start and End Positions: Indicating the exact location of the entity in the question. - Label Type: Denoting whether the entity is a table, column, or value. - Lexeme: The original entity extracted from the SQL query. - Similarity Score: Reflecting the confidence of the match.\n\n\n3.4 Applying the BIO Tagging\nFinally, we convert the annotated entities into a BIO tagging format:\n\nB-Label: Marks the beginning of an entity.\nI-Label: Marks tokens inside an entity.\nO: Marks tokens outside any entity."
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#step-by-step-example",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#step-by-step-example",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "4 Step-by-Step Example",
    "text": "4 Step-by-Step Example\n\n4.0.1 Extracting Lexemes\nUsing pglast, we parse the SQL query and extract the following lexemes:\n\nannotator = PglastAnnotator()\n\nannotated_example = annotator.annotate(examples_list[0])\npprint(annotated_example.model_dump())\n\n{'entities': [{'end': 34,\n               'label_type': 'column',\n               'lexeme': 'movie_release_year',\n               'schema_element': None,\n               'similarity': 0.77,\n               'start': 5,\n               'substring': 'movie titles released in year'},\n              {'end': 101,\n               'label_type': 'column',\n               'lexeme': 'movie_popularity',\n               'schema_element': None,\n               'similarity': 1.0,\n               'start': 85,\n               'substring': 'movie popularity'},\n              {'end': 17,\n               'label_type': 'column',\n               'lexeme': 'movie_title',\n               'schema_element': None,\n               'similarity': 0.96,\n               'start': 5,\n               'substring': 'movie titles'},\n              {'end': 10,\n               'label_type': 'table',\n               'lexeme': 'movies',\n               'schema_element': None,\n               'similarity': 0.91,\n               'start': 5,\n               'substring': 'movie'},\n              {'end': 39,\n               'label_type': 'value',\n               'lexeme': '1945',\n               'schema_element': None,\n               'similarity': 1.0,\n               'start': 35,\n               'substring': '1945'}],\n 'id': 'bird:train.json:0',\n 'question': 'Name movie titles released in year 1945. Sort the listing by the '\n             'descending order of movie popularity.'}\n\n\n\n\n4.0.2 Matching Lexemes\nWe use convolutional search with fuzzy matching to align lexemes with segments of the sentence (question text). The matching process identifies the most similar substring within a sliding window across the sentence, based on a similarity threshold:\n\nsentence = \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\"\nlexemes = [\n    ('column', 'movie_release_year'),\n    # ('column', 'movie_popularity'),\n    # ('column', 'movie_title'),\n    # ('table', 'movies'),\n    # ('value', '1945')\n]\nprint(\"Sentence =\",sentence)\nprint(\"lexemes =\", lexemes)\n# Set a similarity threshold\nthreshold = 0.8\n\n# Perform matching\nprint(\"Starting the matching process:\")\nentities = conv_match_substring(sentence, lexemes, threshold=threshold)\n\n# Display the matched entities\nprint(\"\\nMatched entities:\")\nfor entity in entities:\n    print(f\"Entity Type: {entity.label_type}\")\n    print(f\"Matched Text: '{sentence[entity.start:entity.end]}'\")\n    print(f\"Lexeme: {entity.lexeme}\")\n    print(f\"Similarity: {entity.similarity}\\n\")\n\nSentence = Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\nlexemes = [('column', 'movie_release_year')]\nStarting the matching process:\n\nMatching lexeme 'movie_release_year' of type 'column'\n\nSearching for best match for phrase 'movie_release_year' in sentence.\nWindow 'Name movie titles released in' (Tokens 0-5): Similarity = 0.64\nWindow 'movie titles released in year' (Tokens 1-6): Similarity = 0.77\nWindow 'titles released in year 1945' (Tokens 2-7): Similarity = 0.61\nWindow 'released in year 1945. Sort' (Tokens 3-8): Similarity = 0.64\nWindow 'in year 1945. Sort the' (Tokens 4-9): Similarity = 0.46\nWindow 'year 1945. Sort the listing' (Tokens 5-10): Similarity = 0.41\nWindow '1945. Sort the listing by' (Tokens 6-11): Similarity = 0.24\nWindow 'Sort the listing by the' (Tokens 7-12): Similarity = 0.29\nWindow 'the listing by the descending' (Tokens 8-13): Similarity = 0.3\nWindow 'listing by the descending order' (Tokens 9-14): Similarity = 0.29\nWindow 'by the descending order of' (Tokens 10-15): Similarity = 0.32\nWindow 'the descending order of movie' (Tokens 11-16): Similarity = 0.43\nWindow 'descending order of movie popularity' (Tokens 12-17): Similarity = 0.41\nWindow size 5: Best match 'movie titles released in year' with similarity 0.77\nWindow 'Name movie titles released' (Tokens 0-4): Similarity = 0.68\nWindow 'movie titles released in' (Tokens 1-5): Similarity = 0.71\nWindow 'titles released in year' (Tokens 2-6): Similarity = 0.68\nWindow 'released in year 1945' (Tokens 3-7): Similarity = 0.72\nWindow 'in year 1945. Sort' (Tokens 4-8): Similarity = 0.46\nWindow 'year 1945. Sort the' (Tokens 5-9): Similarity = 0.44\nWindow '1945. Sort the listing' (Tokens 6-10): Similarity = 0.26\nWindow 'Sort the listing by' (Tokens 7-11): Similarity = 0.27\nWindow 'the listing by the' (Tokens 8-12): Similarity = 0.33\nWindow 'listing by the descending' (Tokens 9-13): Similarity = 0.28\nWindow 'by the descending order' (Tokens 10-14): Similarity = 0.34\nWindow 'the descending order of' (Tokens 11-15): Similarity = 0.29\nWindow 'descending order of movie' (Tokens 12-16): Similarity = 0.42\nWindow 'order of movie popularity' (Tokens 13-17): Similarity = 0.51\nWindow size 4: Best match 'released in year 1945' with similarity 0.72\nWindow 'Name movie titles' (Tokens 0-3): Similarity = 0.57\nWindow 'movie titles released' (Tokens 1-4): Similarity = 0.77\nWindow 'titles released in' (Tokens 2-5): Similarity = 0.61\nWindow 'released in year' (Tokens 3-6): Similarity = 0.82\nWindow 'in year 1945' (Tokens 4-7): Similarity = 0.4\nWindow 'year 1945. Sort' (Tokens 5-8): Similarity = 0.44\nWindow '1945. Sort the' (Tokens 6-9): Similarity = 0.26\nWindow 'Sort the listing' (Tokens 7-10): Similarity = 0.29\nWindow 'the listing by' (Tokens 8-11): Similarity = 0.31\nWindow 'listing by the' (Tokens 9-12): Similarity = 0.31\nWindow 'by the descending' (Tokens 10-13): Similarity = 0.34\nWindow 'the descending order' (Tokens 11-14): Similarity = 0.32\nWindow 'descending order of' (Tokens 12-15): Similarity = 0.32\nWindow 'order of movie' (Tokens 13-16): Similarity = 0.56\nWindow 'of movie popularity' (Tokens 14-17): Similarity = 0.49\nWindow size 3: Best match 'released in year' with similarity 0.82\nWindow 'Name movie' (Tokens 0-2): Similarity = 0.57\nWindow 'movie titles' (Tokens 1-3): Similarity = 0.6\nWindow 'titles released' (Tokens 2-4): Similarity = 0.55\nWindow 'released in' (Tokens 3-5): Similarity = 0.62\nWindow 'in year' (Tokens 4-6): Similarity = 0.48\nWindow 'year 1945' (Tokens 5-7): Similarity = 0.37\nWindow '1945. Sort' (Tokens 6-8): Similarity = 0.22\nWindow 'Sort the' (Tokens 7-9): Similarity = 0.31\nWindow 'the listing' (Tokens 8-10): Similarity = 0.28\nWindow 'listing by' (Tokens 9-11): Similarity = 0.21\nWindow 'by the' (Tokens 10-12): Similarity = 0.17\nWindow 'the descending' (Tokens 11-13): Similarity = 0.31\nWindow 'descending order' (Tokens 12-14): Similarity = 0.35\nWindow 'order of' (Tokens 13-15): Similarity = 0.38\nWindow 'of movie' (Tokens 14-16): Similarity = 0.46\nWindow 'movie popularity' (Tokens 15-17): Similarity = 0.53\nWindow size 2: Best match 'released in' with similarity 0.62\nWindow 'Name' (Tokens 0-1): Similarity = 0.18\nWindow 'movie' (Tokens 1-2): Similarity = 0.43\nWindow 'titles' (Tokens 2-3): Similarity = 0.33\nWindow 'released' (Tokens 3-4): Similarity = 0.54\nWindow 'in' (Tokens 4-5): Similarity = 0.1\nWindow 'year' (Tokens 5-6): Similarity = 0.36\nWindow '1945' (Tokens 6-7): Similarity = 0.0\nWindow 'Sort' (Tokens 7-8): Similarity = 0.18\nWindow 'the' (Tokens 8-9): Similarity = 0.1\nWindow 'listing' (Tokens 9-10): Similarity = 0.16\nWindow 'by' (Tokens 10-11): Similarity = 0.1\nWindow 'the' (Tokens 11-12): Similarity = 0.1\nWindow 'descending' (Tokens 12-13): Similarity = 0.21\nWindow 'order' (Tokens 13-14): Similarity = 0.35\nWindow 'of' (Tokens 14-15): Similarity = 0.1\nWindow 'movie' (Tokens 15-16): Similarity = 0.43\nWindow 'popularity' (Tokens 16-17): Similarity = 0.29\nWindow size 1: Best match 'released' with similarity 0.54\n\nBest overall match: 'released in year' with similarity 0.82\nMatched 'released in year' in sentence with similarity 0.82\n\nMatched entities:\nEntity Type: column\nMatched Text: 'released in year'\nLexeme: movie_release_year\nSimilarity: 0.82"
  },
  {
    "objectID": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#challenges",
    "href": "articles/Approaches and Challenges in Annotating a Closed Domain NER Dataset.html#challenges",
    "title": "Approaches and Challenges in Annotating a Closed Domain NER Dataset",
    "section": "5 Challenges",
    "text": "5 Challenges\n\nAlignment Issues:\n\nNatural language questions often use varied phrasing that doesn’t directly match the lexemes (e.g., table names, column names) in the database schema.\n\nOverlapping Entities:\n\nWhen multiple entities are mentioned closely together in a question, their textual representations can overlap.\n\n\nWe can approach the challenge in the following ways:\n\nAlignment\n\nContinous Annotation: Only continuous (adjacent) substrings in the sentence can be annotated as entities. This means that the words corresponding to an entity must be next to each other without any interruptions.\nNon-continuous Annotation: Allows for the annotation of entities even if the corresponding words are not adjacent in the sentence. This approach is more flexible and can capture entities that are mentioned in a scattered manner throughout the sentence.\n\nOverlap\n\nOverlap Annotation: Annotations are allowed to overlap in the sentence; that is, a word or phrase can be part of multiple entity annotations. This is useful when different entities share common words in the question.\nNon-overlap Annotation: Annotations cannot overlap; each word or phrase can be assigned to at most one entity. This constraint ensures that once a word is part of an entity annotation, it cannot be part of another.\n\n\n\n5.1 Example\nSuppose the following sentence and lexemes:\n\nsentence = \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\"\nlexemes = [\n    ('column', 'movie_release_year'),\n    ('column', 'movie_popularity'),\n    ('column', 'movie_title'),\n    ('table', 'movies'),\n    ('value', '1945')\n]\n\n\n\n5.2 Continous Overlapping\nDefinition: Only continuous substrings in the sentence can be annotated as entities, and annotations are allowed to overlap (i.e., a word or phrase can be part of multiple annotations).\nSentence: “Name [{movie} titles] [released in year] [1945]. Sort the listing by the descending order of [movie popularity].”\n\n{movie} and [movie released in year] matches to the ‘movie_release_year’ column.\n[movie titles] matches the ‘movie_title’ column.\n[1945] matches the ‘1945’ value.\n[movie popularity] matches the ‘movie_popularity’ column.\n\n\n\n5.3 Continuous Non-Overlapping\nDefinition: Only continuous substrings in the sentence can be annotated as entities, and annotations cannot overlap (i.e., each word or phrase can be part of at most one annotation).\nSentence: “Name [movie titles] [released in year] [1945]. Sort the listing by the descending order of [movie popularity].”\n\n[movie titles] matches the ‘movie_title’ column.\n[released in year] matches the ‘movie_release_year’ column.\n[1945] matches the ‘1945’ value.\n[movie popularity] matches the ‘movie_popularity’ column.\n\n\n\n5.4 Non-Continuous Overlapping\nDefinition: Substrings can be non-continuous (i.e., words corresponding to an entity do not need to be adjacent), and annotations are allowed to overlap.\nSentence: “Name [{movie} titles] [released] in [year] [1945]. Sort the listing by the descending order of [{movie} popularity].”\n\n{movie} matches the ‘movies’ table.\n[movie titles] separately match the ‘movie_title’ columns.\n{movie}, [released] and [year] correspond to the ‘movie_release_year’ columns.\n[1945] matches the ‘1945’ value.\n[movie popularity] correspond to the ‘movie_popularity’ column.\n\n\n\n5.5 Non-Continuous Non-Overlapping\nDefinition: Substrings can be non-continuous, and annotations cannot overlap.\nSentence: “Name [movie] [titles] [released] in [year] [1945]. Sort the listing by the descending order of [movie] [popularity].”\n\n[movie] matches the ‘movies’ table or ‘movie_title’ column.\n[titles] matches the ‘movie_title’ column.\n[released] matches the ‘movie_release_year’ column.\n[year] matches the ‘movie_release_year’ column.\n[1945] matches the ‘1945’ value.\n[movie] [popularity] matches the ‘movie_popularity’ column."
  },
  {
    "objectID": "articles/index.html",
    "href": "articles/index.html",
    "title": "Articles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApproaches and Challenges in Annotating a Closed Domain NER Dataset\n\n\nZikun Fu\n\n\n\n\nStructured Constraint Programming\n\n\nKen Pu\n\n\n\n\n\nNo matching items\n\n\nAI Automation Group, 2024 ©"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "1 Members\n\n\n\nKen Pu is an associate professor in Computer Science at Ontario Tech University. He has been actively working in the intersection of database systems, text and natural language processing, machine learning and artificial intelligence. He received his PhD in Computer Science from University of Toronto in 2006.\n\n\n\n\n\n\n\nLimin Ma is working on AI agents and agentic workflows involving multimodal and custom LLMs. He obtained his Masters in Computer Science from Ontario Tech University where he worked on embedded neural networks for keyword query optimization.\n\n\n\n\n\nZikun Fu is working on instruction tuned embedding models for semantic retrieval applications. He is currently working on benchmarks for instruction based embedding models. Zikun is currently working on his master’s degree.\n\n\n\n\n\nChen Yang is a visiting researcher for the duration of Summer 2024. We are studying text embedding models with instructions: their evaluation and optimization. Chen is completing his master’s degree from Northeasstern University.\n\n\n\n\n\n\n\nAI Automation Group, 2024 ©"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1 Research Statement",
    "section": "",
    "text": "We study artificial intelligence & workflow automation"
  },
  {
    "objectID": "index.html#fundamental-ai-research",
    "href": "index.html#fundamental-ai-research",
    "title": "1 Research Statement",
    "section": "1.1 Fundamental AI research",
    "text": "1.1 Fundamental AI research\nWe aim to advance the foundations of data science, machine learning, and agentic AI.\n\nmathematical models of neural network dynamics\nmeta-learning methods for ensemble learning\nintegration of AI with classical computer science"
  },
  {
    "objectID": "index.html#novel-applications-of-ai",
    "href": "index.html#novel-applications-of-ai",
    "title": "1 Research Statement",
    "section": "1.2 Novel applications of AI",
    "text": "1.2 Novel applications of AI\nOur research focuses on discovering and implementing novel applications of AI technology, particularly in workflow automation. We aim to enhance efficiency, reduce costs, and improve accuracy in various business processes."
  },
  {
    "objectID": "index.html#open-source-benchmarks",
    "href": "index.html#open-source-benchmarks",
    "title": "1 Research Statement",
    "section": "1.3 Open-source benchmarks",
    "text": "1.3 Open-source benchmarks\nWe are committed to providing the research community with open source benchmarks to evaluate AI models and technologies. These benchmarks are crucial for ensuring transparency, reproducibility, and progress in the field of AI research."
  },
  {
    "objectID": "index.html#ai-readiness-report",
    "href": "index.html#ai-readiness-report",
    "title": "1 Research Statement",
    "section": "1.4 AI readiness report",
    "text": "1.4 AI readiness report\nWe assess AI readiness across various industry sectors, identifying opportunities and challenges. Our reports offer insights into the practical implementation of AI technologies and their impact on industry operations."
  }
]